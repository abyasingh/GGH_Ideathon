# -*- coding: utf-8 -*-
"""deforestation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J96InkaOhlFsvBsofOpRkgsr9ndgNNWB
"""

import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import tensorflow_io as tfio
print(tfio.__version__)

from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D
from keras import regularizers
from tensorflow.keras import optimizers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import os
import seaborn as sns
import cv2
import tensorflow as tf
from tensorflow import keras
from matplotlib.image import imread
from sklearn.model_selection import train_test_split
from keras import optimizers
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator

train_path = '/kaggle/input/planet-understanding-the-amazon-from-space/train_v2.csv/train_v2.csv'
test_path = '/kaggle/input/planet-understanding-the-amazon-from-space/sample_submission_v2.csv/sample_submission_v2.csv'
train_images = '/kaggle/input/uploadingagain/train-jpg/train-jpg'
test_images = '/kaggle/input/missedfiles/test-jpg'

train_df = pd.read_csv(train_path)
print(train_df.shape)
train_df.head()

test_df = pd.read_csv(test_path)
print(test_df.shape)
test_df.head()

folder = train_images  # Replace with the correct folder path
for i in range(10):
    filename = folder + '/' + 'train_' + str(i) + '.jpg'
    image = cv2.imread(filename)

    if image is None:
        print(f"Failed to load image: {filename}")
        continue

    # Check image data type
    if image.dtype != np.uint8:
        image = image.astype(np.uint8)

    plt.imshow(image)
    plt.show()

print(f'Number of images: {train_df.shape[0]}')

train_df['tags'].nunique()

tags = train_df['tags'].apply(lambda x: x.split(' '))
tags = [item for sublist in tags for item in sublist]
tag_counts = pd.Series(tags).value_counts()

plt.figure(figsize=(10,6))
plt.bar(tag_counts.index, tag_counts.values, alpha=0.8)
plt.title('Tag counts')
plt.ylabel('Number of occurrences', fontsize=12)
plt.xlabel('Tags', fontsize=12)
plt.xticks(rotation=90)
plt.show();

labels = set()
def splitting_tags(tags):
    [labels.add(tag) for tag in tags.split()]

train_df1 = train_df.copy()
train_df1['tags'].apply(splitting_tags)
labels = list(labels)
print(labels)

for tag in labels:
    train_df1[tag] = train_df1['tags'].apply(lambda x: 1 if tag in x.split() else 0)

train_df1['image_name'] = train_df1['image_name'].apply(lambda x: '{}.jpg'.format(x))
train_df1.head()

columns = list(train_df1.columns[2:])
train_datagen = ImageDataGenerator(rescale = 1./255., validation_split = 0.2)

train_generator = train_datagen.flow_from_dataframe(dataframe=train_df1,
                                                    directory =train_images,
                                                    x_col='image_name', y_col=columns, subset='training',
                                                    batch_size=32,seed=42, shuffle=True,
                                                    class_mode='raw', target_size=(128,128))

val_generator = train_datagen.flow_from_dataframe(dataframe=train_df1,
                                                  directory =train_images,
                                                  x_col='image_name', y_col=columns, subset='validation',
                                                  batch_size=32,seed=42, shuffle=True,
                                                  class_mode='raw', target_size=(128,128))

step_train_size = int(np.ceil(train_generator.samples / train_generator.batch_size))
step_val_size = int(np.ceil(val_generator.samples / val_generator.batch_size))

def cnn_model():
    model = Sequential()

    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(128, 128, 3)))
    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())

    model.add(Dense(units=512, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=256, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=128, activation='relu'))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=17, activation='sigmoid'))

    model.compile(optimizer=Adam(lr=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    return model

model = cnn_model()

loss_function = tf.keras.losses.BinaryCrossentropy()
optimizer = Adam(lr=0.001)

loss_values = []

history = model.fit(x=train_generator,
                    steps_per_epoch=step_train_size,
                    validation_data=val_generator,
                    validation_steps=step_val_size,
                    epochs=20,
                    callbacks=[tf.keras.callbacks.LambdaCallback(
                        on_epoch_end=lambda epoch, logs: loss_values.append(logs['loss']))
                    ])

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['training', 'validation'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['training', 'validation'], loc='upper left')

plt.show()

df_loss = pd.DataFrame({'loss': loss_values})

# Plot the loss function
plt.plot(df_loss.index, df_loss['loss'])
plt.xlabel('Iterations or Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

sample_submission = pd.read_csv('/kaggle/input/planet-understanding-the-amazon-from-space/sample_submission_v2.csv/sample_submission_v2.csv')
sample_submission1 = sample_submission.copy()
sample_submission1['image_name'] = sample_submission1['image_name'].apply(lambda x: '{}.jpg'.format(x))
sample_submission1.head()

test_df1 = sample_submission1.iloc[:40669]['image_name'].reset_index().drop('index', axis =1)
test_df1.head()

test_datagen = ImageDataGenerator(rescale = 1/255)

test_gen = test_datagen.flow_from_dataframe(dataframe=test_df1,
                                            directory='/kaggle/input/missedfiles/test-jpg',
                                            x_col="image_name",
                                            y_col=None,
                                            batch_size=32,
                                            seed=42,
                                            shuffle=False,
                                            class_mode=None,
                                            target_size=(128,128))

step_test_size1 = int(np.ceil(test_gen.samples/test_gen.batch_size))

test_gen.reset()
pred = model1.predict(test_gen, steps=step_test_size1, verbose=1)

file_names = test_gen.filenames

pred_tags = pd.DataFrame(pred)
pred_tags = pred_tags.apply(lambda x: ' '.join(np.array(labels)[x > 0.5]), axis = 1)

result1 = pd.DataFrame({'image_name': file_names, 'tags': pred_tags})
result1.head()

result1.to_csv('predictionss.csv', index = False)

